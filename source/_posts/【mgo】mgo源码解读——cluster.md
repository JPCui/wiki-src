---
title: ã€mgoã€‘mgoæºç è§£è¯»â€”â€”cluster
date: 2018-11-05 15:11:52
tags:
  - golang
  - mgo
---
## å®šæ—¶æ›´æ–°æœåŠ¡å™¨ä¿¡æ¯ â€”â€” syncServersLoop

```
// syncServersLoop loops while the cluster is alive to keep its idea of the server topology up-to-date.
// å½“é›†ç¾¤å¤„äºæ´»åŠ¨çŠ¶æ€æ—¶ï¼Œä»¥å¾ªç¯æ¥ä¿æŒæœåŠ¡çš„å®æ—¶æ‹“æ‰‘ç»“æ„ã€‚

// It must be called just once from newCluster.
// å¿…é¡»ä» newCluster è°ƒç”¨ä¸€æ¬¡ã€‚

// The loop iterates once syncServersDelay has passed, or if somebody injects a value into the cluster.sync channel to force a synchronization.
// ä¸€æ®µæ—¶é—´ï¼ˆsyncServersDelayï¼‰çš„å»¶è¿Ÿè¿‡åï¼Œå¾ªç¯ä¼šè¿›è¡Œä¸€æ¬¡è¿­ä»£ã€‚æˆ–è€…åœ¨ cluster.sync ç®¡é“ä¸­æ³¨å…¥ä¸€ä¸ªå€¼ï¼Œæ¥å¼ºåˆ¶æ‰§è¡Œä¸€æ¬¡åŒæ­¥æ“ä½œã€‚ï¼ˆä»æœ€åçš„selectç®¡é“é€šä¿¡å¯ä»¥å¾ˆå®¹æ˜“çœ‹å‡ºï¼‰

// A loop iteration will contact all servers in parallel, ask them about known peers and their own role within the cluster, and then attempt to do the same with all the peers retrieved.
// ä¸€æ¬¡å¾ªç¯è¿­ä»£å°†ä¼šâœ¨å¹¶è¡Œâœ¨è®¿é—®æ‰€æœ‰æœåŠ¡ï¼ŒæŸ¥è¯¢é›†ç¾¤ä¸­å·²çŸ¥çš„å¯¹ç­‰æœåŠ¡åŠè‡ªå·±çš„è§’è‰²ï¼Œç„¶åå¯¹æ‰€æœ‰å¯¹ç­‰æœåŠ¡å°è¯•åŒæ ·çš„æ“ä½œã€‚
 
func (cluster *mongoCluster) syncServersLoop() {
    // âœ¨ æ­»å¾ªç¯
	for {
		debugf("SYNC Cluster %p is starting a sync loop iteration.", cluster)

		// âœ¨ ä¸Šé”ï¼
		cluster.Lock()
		// âœ¨ é›†ç¾¤ä¸­æ²¡æœ‰æœåŠ¡äº†ï¼ˆå…¨æŒ‚äº†ï¼Ÿï¼‰
		if cluster.references == 0 {
			cluster.Unlock()
			break
		}
		cluster.references++ // Keep alive while syncing.
		// âœ¨ ï¼Ÿï¼Ÿï¼Ÿ
		direct := cluster.direct
		cluster.Unlock()

		cluster.syncServersIteration(direct)

		// We just synchronized, so consume any outstanding requests.
		select {
		case <-cluster.sync:
		default:
		}

		cluster.Release()

		// Hold off before allowing another sync. No point in
		// burning CPU looking for down servers.
		if !cluster.failFast {
			time.Sleep(syncShortDelay)
		}

		cluster.Lock()
		if cluster.references == 0 {
			cluster.Unlock()
			break
		}
		cluster.syncCount++
		// Poke all waiters so they have a chance to timeout or
		// restart syncing if they wish to.
		cluster.serverSynced.Broadcast()
		// Check if we have to restart immediately either way.
		restart := !direct && cluster.masters.Empty() || cluster.servers.Empty()
		cluster.Unlock()

		if restart {
			log("SYNC No masters found. Will synchronize again.")
			time.Sleep(syncShortDelay)
			continue
		}

		debugf("SYNC Cluster %p waiting for next requested or scheduled sync.", cluster)

		// Hold off until somebody explicitly requests a synchronization
		// or it's time to check for a cluster topology change again.
		select {
		case <-cluster.sync:
		case <-time.After(syncServersDelay):
		}
	}
	debugf("SYNC Cluster %p is stopping its sync loop.", cluster)
}
```

## åŒæ­¥æœåŠ¡infoçš„æ ¸å¿ƒ

è®¾è®¡åˆ°çš„ç‚¹

- sync.WaitGroup å¹¶å‘æ§åˆ¶
- sync.Mutex è¿™ä¸ªç®€å•

```
func (cluster *mongoCluster) syncServersIteration(direct bool) {
	log("SYNC Starting full topology synchronization...")

	var wg sync.WaitGroup
	var m sync.Mutex
	notYetAdded := make(map[string]pendingAdd)
	addIfFound := make(map[string]bool)
	seen := make(map[string]bool)
	syncKind := partialSync

	var spawnSync func(addr string, byMaster bool)
	spawnSync = func(addr string, byMaster bool) {
		// âœ¨ æ ‡è®°å¼€å¯äº†ä¸€ä¸ªæ–°çº¿ç¨‹
		wg.Add(1)
		go func() {
			defer wg.Done()

			// âœ¨ è§£æåœ°å€ï¼Œè¿™ä¸ªresolveAddrå®ç°å¥½ğŸ‚ğŸºï¼Œè€ƒè™‘çš„è¾ƒå…¨é¢
			// âœ¨ ï¼ˆä¸è¿›å»çœ‹ä»£ç äº†ï¼‰å…ˆè§£æipåœ°å€ï¼Œä¸æ˜¯ipåœ°å€å°±å½“åˆudpè¿æ¥è§£æï¼Œå› ä¸ºæ‹¨å·è¿æ¥å¯èƒ½è€—æ—¶ï¼Œç”¨äº†selectç®¡é“å¹¶å‘å¤„ç†ï¼Œè¿™é‡Œåˆ†åˆ«ä»¥udp4ã€udp6è¿›è¡Œå°è¯•
			tcpaddr, err := resolveAddr(addr)
			if err != nil {
				log("SYNC Failed to start sync of ", addr, ": ", err.Error())
				return
			}
			resolvedAddr := tcpaddr.String()

			// âœ¨ å·²å‘ç°åœ°å€çš„é€»è¾‘ï¼Œè¦åŠ é”
			m.Lock()
			if byMaster {
				if pending, ok := notYetAdded[resolvedAddr]; ok {
					delete(notYetAdded, resolvedAddr)
					m.Unlock()
					cluster.addServer(pending.server, pending.info, completeSync)
					return
				}
				addIfFound[resolvedAddr] = true
			}
			// âœ¨ å¦‚æœå·²ç»å‘ç°è¯¥åœ°å€ï¼Œstop
			if seen[resolvedAddr] {
				m.Unlock()
				return
			}
			// âœ¨ æ ‡è®°å½“å‰åœ°å€å·²è¢«å‘ç°
			seen[resolvedAddr] = true
			m.Unlock()

			server := cluster.server(addr, tcpaddr)
			// âœ¨ åŒæ­¥serverä¿¡æ¯
			info, hosts, err := cluster.syncServer(server)
			if err != nil {
				// âœ¨ åŒæ­¥ä¿¡æ¯å‡ºé”™ï¼Œç§»é™¤
				cluster.removeServer(server)
				return
			}

			m.Lock()
			add := direct || info.Master || addIfFound[resolvedAddr]
			if add {
				syncKind = completeSync
			} else {
				notYetAdded[resolvedAddr] = pendingAdd{server, info}
			}
			m.Unlock()
			if add {
				cluster.addServer(server, info, completeSync)
			}
			if !direct {
				for _, addr := range hosts {
					spawnSync(addr, info.Master)
				}
			}
		}()
	}

	knownAddrs := cluster.getKnownAddrs()
	for _, addr := range knownAddrs {
		spawnSync(addr, false)
	}
	wg.Wait()

	if syncKind == completeSync {
		logf("SYNC Synchronization was complete (got data from primary).")
		for _, pending := range notYetAdded {
			cluster.removeServer(pending.server)
		}
	} else {
		logf("SYNC Synchronization was partial (cannot talk to primary).")
		for _, pending := range notYetAdded {
			cluster.addServer(pending.server, pending.info, partialSync)
		}
	}

	cluster.Lock()
	mastersLen := cluster.masters.Len()
	logf("SYNC Synchronization completed: %d master(s) and %d slave(s) alive.", mastersLen, cluster.servers.Len()-mastersLen)

	// Update dynamic seeds, but only if we have any good servers. Otherwise,
	// leave them alone for better chances of a successful sync in the future.
	if syncKind == completeSync {
		dynaSeeds := make([]string, cluster.servers.Len())
		for i, server := range cluster.servers.Slice() {
			dynaSeeds[i] = server.Addr
		}
		cluster.dynaSeeds = dynaSeeds
		debugf("SYNC New dynamic seeds: %#v\n", dynaSeeds)
	}
	cluster.Unlock()
}
```